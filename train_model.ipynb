{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737918cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf7d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv.VideoCapture(0)\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     ret,frame = cap.read()\n",
    "    \n",
    "#     cv.imshow(\"frame\",frame)\n",
    "    \n",
    "#     if cv.waitKey(2) & 0xFF == ord('x'):\n",
    "#         break\n",
    "# cap.release()\n",
    "# cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95094a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a29f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Inceptionresnet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b6691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import InceptionResNetV2\n",
    "import cv2 as cv\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d53c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inception_resnet(\n",
    "    weights=\"imagenet\", input_shape=(160, 160, 3), include_top=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the InceptionResNetV2 model with specific configurations.\n",
    "    :param weights: Pre-trained weights or path to custom weights.\n",
    "    :param input_shape: Input image shape (default is (160, 160, 3)).\n",
    "    :param include_top: Whether to include the top layers.\n",
    "    :return: InceptionResNetV2 model instance.\n",
    "    \"\"\"\n",
    "    return InceptionResNetV2(\n",
    "        weights=weights, input_shape=input_shape, include_top=include_top\n",
    "    )\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "facenet_encoder = load_inception_resnet()\n",
    "facenet_encoder.load_weights(\"utils/facenet_keras_weights.h5\", by_name=True)\n",
    "FRmodel = facenet_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f4adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FRmodel.inputs)\n",
    "print(FRmodel.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5e0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec44cd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "def img_to_encoding(image, model):\n",
    "    if image is None:\n",
    "        return 0\n",
    "    else:\n",
    "        img = cv.resize(image,(160,160))\n",
    "        img = np.around(np.asarray(img)/255.0, decimals=12)\n",
    "        x_train = np.expand_dims(img, axis=0)\n",
    "        embedding = model.predict_on_batch(x_train)\n",
    "        return embedding / np.linalg.norm(embedding, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce62c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_encoding(image, model):\n",
    "    \"\"\"\n",
    "    Converts the input image to an encoding using the given model.\n",
    "\n",
    "    Arguments:\n",
    "    image -- the input image\n",
    "    model -- the face recognition model\n",
    "\n",
    "    Returns:\n",
    "    encoding -- the normalized face encoding\n",
    "    \"\"\"\n",
    "    # Resize the image to the expected input size (160x160)\n",
    "    img = cv.resize(image, (160, 160))\n",
    "    \n",
    "    # Normalize pixel values between 0 and 1\n",
    "    img = np.around(np.asarray(img) / 255.0, decimals=12)\n",
    "\n",
    "    # Get the embedding from the model\n",
    "    embedding = model.predict_on_batch(np.expand_dims(img, axis=0))\n",
    "\n",
    "    # Flatten the embedding to ensure it's a 1D array\n",
    "    embedding = embedding.flatten()\n",
    "\n",
    "    # Print the shape of the embedding to understand its dimensions\n",
    "    print(\"Embedding shape:\", embedding.shape)\n",
    "\n",
    "    # Normalize the embedding vector\n",
    "    norm_embedding = np.linalg.norm(embedding, ord=2)\n",
    "\n",
    "    # If norm is zero, return a zero embedding or handle it gracefully\n",
    "    if norm_embedding == 0:\n",
    "        print(\"Warning: Zero embedding detected\")\n",
    "        return embedding  # Return the original embedding, or handle as needed\n",
    "\n",
    "    return embedding / norm_embedding\n",
    "\n",
    "\n",
    "# Load images and create a database of encodings\n",
    "filepath = \"extract_face/\"\n",
    "database = {}\n",
    "\n",
    "\n",
    "def load_image(filepath):\n",
    "    \"\"\"\n",
    "    Load images from the specified directory and encode them into the database.\n",
    "    :param filepath: Directory containing folders of images.\n",
    "    :return: A dictionary where keys are folder names (identities) and values are lists of encodings.\n",
    "    \"\"\"\n",
    "    for folder in os.listdir(filepath):\n",
    "        encodings = []\n",
    "        for subfolder in os.listdir(filepath + folder):\n",
    "            image_BGR = cv.imread(filepath + folder + \"/\" + subfolder)\n",
    "            if image_BGR is not None:\n",
    "                temp = img_to_encoding(image_BGR, FRmodel)\n",
    "                encodings.append(temp)\n",
    "        database[folder] = encodings\n",
    "    return database\n",
    "\n",
    "\n",
    "# Load the image database\n",
    "database = load_image(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf88bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "database[\"Ketul\"][10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a75bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(database.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaafc7a9",
   "metadata": {},
   "source": [
    "# Face Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce155c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#face verification\n",
    "def verify(image, identity, database, model):\n",
    "    \"\"\"\n",
    "    Function that verifies if the person on the \"image_path\" image is \"identity\".\n",
    "    \n",
    "    Arguments:\n",
    "        image_path -- path to an image\n",
    "        identity -- string, name of the person you'd like to verify the identity. Has to be an employee who works in the office.\n",
    "        database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n",
    "        model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "        dist -- distance between the image_path and the image of \"identity\" in the database.\n",
    "        door_open -- True, if the door should open. False otherwise.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    # Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (≈ 1 line)\n",
    "#     image_RGB = cv.cvtColor(image,cv.COLOR_BGR2RGB)\n",
    "#     pixel = detector.detect_faces(image_RGB)\n",
    "#     x,y,w,h = pixel[0]['box']\n",
    "#     img = image_RGB[y:y+h,x:x+w]\n",
    "    encoding = img_to_encoding(image,model)\n",
    "    for i in database[identity]:\n",
    "        # Step 2: Compute distance with identity's image (≈ 1 line)\n",
    "        print(i)\n",
    "        dist = np.linalg.norm(encoding - i)\n",
    "        # Step 3: Open the door if dist < 0.7, else don't open (≈ 3 lines)\n",
    "        if dist<0.6:\n",
    "            print(\"It's \" + str(identity) + \", welcome in!\")\n",
    "            door_open = True\n",
    "            break\n",
    "        else:\n",
    "            print(\"It's not \" + str(identity) + \", please go away\")\n",
    "            door_open = False\n",
    "            break\n",
    "    ### END CODE HERE        \n",
    "    return dist, door_open,encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7df473",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance, door_open_flag, encoding = verify(cv.imread(\"extract_face/Ketul/IMG20241127160112_BURST005_32714670.jpg\"), \"Ketul\", database, FRmodel)\n",
    "print(\"(\", distance, \",\", door_open_flag, \")\")\n",
    "print(encoding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170bdb54",
   "metadata": {},
   "source": [
    "# Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437979dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def who_is_it(image, database, model):\n",
    "    \"\"\"\n",
    "    Implements face recognition for the office by finding who is the person on the image_path image.\n",
    "\n",
    "    Arguments:\n",
    "        image_path -- path to an image\n",
    "        database -- database containing image encodings along with the name of the person on the image\n",
    "        model -- your Inception model instance in Keras\n",
    "\n",
    "    Returns:\n",
    "        min_dist -- the minimum distance between image_path encoding and the encodings from the database\n",
    "        identity -- string, the name prediction for the person on image_path\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Compute the target \"encoding\" for the image\n",
    "    encoding = img_to_encoding(image, model)\n",
    "\n",
    "    # Step 2: Find the closest encoding\n",
    "    min_dist = 100  # Initialize \"min_dist\" to a large value\n",
    "    identity = (\n",
    "        None  # Initialize identity to None, to handle case where no match is found\n",
    "    )\n",
    "\n",
    "    # Loop over the database dictionary's names and encodings\n",
    "    for name, db_enc in database.items():\n",
    "        for i in db_enc:\n",
    "            # Compute L2 distance between the target \"encoding\" and the current db_enc from the database\n",
    "            dist = np.linalg.norm(encoding - i)\n",
    "\n",
    "            # If this distance is less than the min_dist, then set min_dist to dist, and identity to name\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                identity = name  # Set identity to the matching name\n",
    "\n",
    "    # If no match was found (identity is None), handle it\n",
    "    if identity is None:\n",
    "        print(\"No match found!\")\n",
    "    else:\n",
    "        if min_dist > 0.55:\n",
    "            print(\"No match with sufficient confidence\")\n",
    "        else:\n",
    "            print(f\"It's {identity}, the distance is {min_dist}\")\n",
    "\n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d7f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_RGB = cv.cvtColor(cv.imread(\"Data/val/jay/12.jpg\"),cv.COLOR_BGR2RGB)\n",
    "# pixel = detector.detect_faces(image_RGB)\n",
    "# x,y,w,h = pixel[0]['box']\n",
    "# img = image_RGB[y:y+h,x:x+w]\n",
    "img = cv.imread(\"extract_face/Ketul/IMG20241127160112_BURST005_32714670.jpg\")\n",
    "# color = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "# face = haar_casecade.detectMultiScale(color,\n",
    "#                                           scaleFactor=1.3,\n",
    "#                                          minNeighbors=5,)\n",
    "# for (x,y,w,h) in face:\n",
    "#     image = img[y:y+h,x:x+w]\n",
    "test1 = who_is_it(img, database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb87c5",
   "metadata": {},
   "source": [
    "#  Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0fa66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "haar_casecade = cv.CascadeClassifier(\"utils/haar_face.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd1cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mtcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5e1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ccaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = MTCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fa6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "!py -3.9 -m pip install face_recognition\n",
    "import face_recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34442bd6",
   "metadata": {},
   "source": [
    "# Face Detection and Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78eba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv.VideoCapture(0)\n",
    "# fourcc = cv.VideoWriter_fourcc(*'XVID')\n",
    "# out = cv.VideoWriter(\"output.mp4\",fourcc,10.00,(640,480))\n",
    "\n",
    "while True:\n",
    "    content,frame = cap.read()\n",
    "    gray = cv.cvtColor(frame,cv.COLOR_BGR2GRAY)\n",
    "#     image_RGB = cv.cvtColor(frame,cv.COLOR_BGR2RGB)\n",
    "    faces = haar_casecade.detectMultiScale(gray,\n",
    "                                          scaleFactor=1.3,\n",
    "                                          minNeighbors=4,\n",
    "                                        )\n",
    "    \n",
    "#     face = detector.detect_faces(image_RGB)\n",
    "#     if face != []:\n",
    "#         box = np.array([face[0]['box']])\n",
    "    \n",
    "#     face_detector = dlib.get_frontal_face_detector()\n",
    "#     landmark_predictor = dlib.shape_predictor(\"utils/shape_predictor_68_face_landmarks.dat\")\n",
    "    \n",
    "    #faces = face_detector(gray)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        \n",
    "        #x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "        cv.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),thickness=2)\n",
    "        # Extract the face region\n",
    "        face1 = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        \n",
    "        # Perform face alignment using facial landmarks\n",
    "        face_locations = face_recognition.face_locations(frame)\n",
    "        face_landmarks = face_recognition.face_landmarks(frame, face_locations)\n",
    "        #print(face_landmarks)\n",
    "\n",
    "#         for landmarks in face_landmarks:\n",
    "#             # Get coordinates of left eye, right eye, and nose tip\n",
    "#             left_eye = landmarks['left_eye']\n",
    "#             right_eye = landmarks['right_eye']\n",
    "#             nose_tip = landmarks['nose_tip']\n",
    "\n",
    "#             # Calculate the center of mass for the eyes\n",
    "#             left_eye_center = np.mean(left_eye, axis=0).astype(int)\n",
    "#             right_eye_center = np.mean(right_eye, axis=0).astype(int)\n",
    "\n",
    "#             # Calculate the angle between the eyes\n",
    "#             angle = np.degrees(np.arctan2(right_eye_center[1] - left_eye_center[1], right_eye_center[0] - left_eye_center[0]))\n",
    "\n",
    "#             # Perform rotation to align the face\n",
    "#             rotation_matrix = cv.getRotationMatrix2D(tuple(nose_tip[0]), angle, scale=1)\n",
    "#             aligned_face = cv.warpAffine(face, rotation_matrix, (w, h), flags=cv.INTER_LINEAR)\n",
    "\n",
    "            \n",
    "        mindist,identity = who_is_it(face1,database,FRmodel)\n",
    "\n",
    "        if mindist<0.6:\n",
    "            cv.putText(frame,identity,(x,y-10),cv.FONT_HERSHEY_PLAIN,2,(0,255,0),2,cv.LINE_AA)           \n",
    "#         else:\n",
    "#             cv.putText(frame,'Unknown',(x,y-10),cv.FONT_HERSHEY_PLAIN,2,(0,0,255),2,cv.LINE_AA)\n",
    "            \n",
    "    #out.write(frame)\n",
    "\n",
    "\n",
    "    cv.imshow(\"video\",frame)\n",
    "    \n",
    "    if cv.waitKey(2) & 0xFF == ord('x'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "#out.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d5520d",
   "metadata": {},
   "source": [
    "# Face Detection Using Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ed6f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "cap = cv.VideoCapture(0)\n",
    "with mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5) as face_detector:\n",
    "    frame_counter = 0\n",
    "    fonts = cv.FONT_HERSHEY_PLAIN\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        frame_counter += 1\n",
    "        ret, frame = cap.read()\n",
    "        if ret is False:\n",
    "            break\n",
    "        rgb_frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "        results = face_detector.process(rgb_frame)\n",
    "        frame_height, frame_width, c = frame.shape\n",
    "        if results.detections:\n",
    "            for face in results.detections:\n",
    "                face_react = np.multiply(\n",
    "                    [\n",
    "                        face.location_data.relative_bounding_box.xmin,\n",
    "                        face.location_data.relative_bounding_box.ymin,\n",
    "                        face.location_data.relative_bounding_box.width,\n",
    "                        face.location_data.relative_bounding_box.height,\n",
    "                    ],\n",
    "                    [frame_width, frame_height, frame_width, frame_height]).astype(int)\n",
    "                #print(face_react)\n",
    "                x = face_react[0]\n",
    "                y = face_react[1]\n",
    "                w = face_react[2]\n",
    "                h = face_react[3]\n",
    "                face = frame[y:y+h,x:x+w]\n",
    "                \n",
    "                cv.rectangle(frame, face_react, color=(255, 255, 255), thickness=2)\n",
    "#                 key_points = np.array([(p.x, p.y) for p in face.location_data.relative_keypoints])\n",
    "#                 key_points_coords = np.multiply(key_points,[frame_width,frame_height]).astype(int)\n",
    "#                 for p in key_points_coords:\n",
    "#                     cv.circle(frame, p, 4, (255, 255, 255), 2)\n",
    "#                     cv.circle(frame, p, 2, (0, 0, 0), -1)\n",
    "                mindist,identity = who_is_it(face,database,FRmodel)\n",
    "\n",
    "                if mindist<0.55:\n",
    "                    cv.putText(frame,identity,(x,y-10),cv.FONT_HERSHEY_PLAIN,2,(0,255,0),2,cv.LINE_AA)           \n",
    "#                 else:\n",
    "#                     cv.putText(frame,'Unknown',(x,y-10),cv.FONT_HERSHEY_PLAIN,2,(0,0,255),2,cv.LINE_AA)\n",
    "        \n",
    "        fps = frame_counter / (time.time() - start_time)\n",
    "        cv.putText(frame,f\"FPS: {fps:.2f}\",(30, 30),cv.FONT_HERSHEY_DUPLEX,0.7,(0, 255, 255),2,)\n",
    "        cv.imshow(\"frame\", frame)\n",
    "        \n",
    "        if cv.waitKey(1) & 0xFF == ord(\"x\"):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9428e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "\n",
    "FRmodel.save(\"saved_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb435faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into tflite\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the Keras model\n",
    "loaded_model = load_model('saved_model.h5')\n",
    "\n",
    "# Convert the model to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open('saved_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c78fc05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da16ef8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
